{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text(filepath, text=[]):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    pdf_reader = PyPDF2.PdfReader(filepath)\n",
    "\n",
    "    # Get the number of pages in the PDF file\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Loop over each page and extract the text\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text().split(\"\\n\")\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text(\"/Users/senthil/Desktop/Senthil/my_testing/Project_Creation/testing_briefs/purchase_orders20230803 (1).pdf\")\n",
    "\n",
    "\n",
    "# for txt in pdf_text:\n",
    "#     print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/senthil/Desktop/Senthil/my_testing/Project_Creation/testing_briefs/purchase_orders 00185079pdf.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchase Order\tTrade Typesetting (match design)\tBloomsbury\tPublishing\tBill To:\tBloomsbury Publishing (UK)\t50 Bedford Square, London, WC1B\t3DP\tTelephone: (0)20 7631 5600\tFax: (0)20 7631 5800\tEmail:\tbloomsburyaccounts@bloomsbury.com\tTo:  De anta - Trade - UK owne d\t51 Clontarf Road\tDublin 3\tContact Simon Eaton, Email: Simon.Eaton@bloomsbury.com\tPO Numbe r00185079\tDate 07/08/2023\tTitleWild Woman\tAuthorPhilippa Forrester\tOther Suppliers\tPrinter/Binder: CPI Trade\tEdition I SBNPr ic e\tISBN:9781399400879\tCommon Specification\tProduct TypeBook\tExtent272\tFormat216 x 135mm - trimmed. 216mm (81/ \") (h) x 135mm (55/  \") (w)\t2 16\tPreparation & TextText Print: 1x1. Print From: Disk.\tPage ComplexityMedium\tImages printing on text paperImages printing on text paper - TBC.\tFoil Blocking - StandardFoil Blocking - Standard - TBC (using OTTO Wolves finishes as guidance for now).\tNotes\tPlease supply Trade Typesetting services for this title, according to the workflow and terms defined in our Trade Typesetting Service Level Agreement\t(‘Agreement’).\tSource files supplied via email\tThey comprise:\t• copy-edited MS\tPages should be designed as follows:\tMatch design of: Groundbreakers 9781399401630\tSpace breaks have been highlighted throughout\tEpigraph Yes, on a standalone page before the Prologue\tContents page Yes, supplied.\tMaps or prelim illustrative material: No\tPart title pages No\tChapter titles Yes: chapter numbers with short chapter titles below\tPage extent (must be divisible by 16): Target page extent: 272pp\tAcknowledgments Fit on one page\tPages to leave blank for material to follow: 6pp for index\tWord count: 70,500\tDigital Bundle information:\tPlease refer to the MS imprint page for ePDF, ePub, and XML ISBNs.\tThe following source elements will be supplied on approval of the final print-ready PDF:\t• cover image\t• plate section (where relevant)\t• author ID\t• author biography\t• cover copy\tKey deadlines:\tCast off and sample pages: 10/8/23, per SLA\t1st pass proofs: 21/8/23, per SLA [Standard]\tNOTE: Bloomsbury Publishing Pic hereby  confirms that no software/font licenses  are assigned,  transferred or sub-licensed to you by the supply of the materials delivered to you for the purposes of performing  the services detailed  in the Agreement  and further  in this Purchase Order.\tCost Breakdown Cost Ref Notes Tot al\tCommon  costs\tTypesetting  (272 pages)@ £2.2000 plus £2.2000  Per Page, runon £2.2000\tTotal\t£600.60 GBP\t£600.60 GBP\tSigned on behalf of Bloomsbury Wildlife -Bloomsbury Publishing (UK)\tSimon Eaton\n",
      "harback ISBN: 9781399400879\n",
      "harback ISBN: ePub, and XML ISBNs.\n",
      "╒═════════════════════════╤════════════════════════════╕\n",
      "│ Key                     │ Metadata                   │\n",
      "╞═════════════════════════╪════════════════════════════╡\n",
      "│ book_title              │ Wild Woman                 │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ author                  │ Philippa Forrester         │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ isbn                    │ 9781399400879              │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ ebook_master            │ ePub,andXMLs.              │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ page_count              │ 272                        │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ text_design             │ )                          │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ trim_size               │ 16 x 135                   │\n",
      "├─────────────────────────┼────────────────────────────┤\n",
      "│ productioneditor_mailid │ Simon.Eaton@bloomsbury.com │\n",
      "╘═════════════════════════╧════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "import docx, re\n",
    "import tabulate as tab\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def extract_docx_to_list(file_path):\n",
    "    # Open the Word document\n",
    "    doc = docx.Document(file_path)\n",
    "\n",
    "    # Initialize an empty list to store extracted text\n",
    "    docx_para_list = []\n",
    "\n",
    "    # Iterate through each element in the document\n",
    "    for element in doc.element.xpath('.//*'):\n",
    "        # Check if the element is a paragraph\n",
    "        if element.tag.endswith('}p'):\n",
    "            # Skip paragraphs that are inside table cells\n",
    "            if element.getparent().tag.endswith('}tc'):\n",
    "                continue\n",
    "            # Get all text elements in the paragraph\n",
    "            text_elements = element.xpath('.//w:t')\n",
    "            if text_elements:\n",
    "                # Concatenate the text from all text elements\n",
    "                text = ''.join([text_element.text for text_element in text_elements])\n",
    "                docx_para_list.append(text)\n",
    "\n",
    "        # Check if the element is a table\n",
    "        elif element.tag.endswith('}tbl'):\n",
    "            # Iterate through each row in the table\n",
    "            for row in element.xpath('.//w:tr'):\n",
    "                # Iterate through each cell in the row\n",
    "                for cell in row.xpath('.//w:tc'):\n",
    "                    cell_text = []\n",
    "                    # Get all paragraphs in the cell\n",
    "                    for paragraph in cell.xpath('.//w:p'):\n",
    "                        runs = paragraph.xpath('.//w:r')\n",
    "                        cell_text_by_paragraph = \"\"\n",
    "                        # Concatenate the text from all runs in each paragraph\n",
    "                        for run in runs:\n",
    "                            run_text = run.xpath('.//w:t')\n",
    "                            if run_text:\n",
    "                                cell_text_by_paragraph += run_text[0].text\n",
    "                        cell_text.append(cell_text_by_paragraph)\n",
    "\n",
    "                    # Check if the cell contains a dropdown list\n",
    "                    result_val = cell.xpath('.//w:r/w:fldChar/w:ffData/w:ddList/w:result/@w:val')\n",
    "                    if result_val:\n",
    "                        selected_value = cell.xpath('.//w:ffData/w:ddList/w:listEntry/@w:val')\n",
    "                        if selected_value:\n",
    "                            # Iterate through each paragraph in the cell and add the selected value for each dropdown to the list\n",
    "                            for i, cell_text_by_paragraph in enumerate(cell_text):\n",
    "                                if i < len(result_val) and i < len(selected_value):\n",
    "                                    docx_para_list.append(f\"{cell_text_by_paragraph} {selected_value[int(result_val[i])]}\")\n",
    "                                else:\n",
    "                                    docx_para_list.append(cell_text_by_paragraph)\n",
    "                        else:\n",
    "                            # Iterate through each paragraph in the cell and add its text to the list\n",
    "                            for cell_text_by_paragraph in cell_text:\n",
    "                                docx_para_list.append(cell_text_by_paragraph)\n",
    "                    else:\n",
    "                        # Iterate through each paragraph in the cell and add its text to the list\n",
    "                        for cell_text_by_paragraph in cell_text:\n",
    "                            docx_para_list.append(cell_text_by_paragraph)\n",
    "\n",
    "    return docx_para_list\n",
    "# EEP\n",
    "\n",
    "text = extract_docx_to_list(file_path)\n",
    "\n",
    "# text = text +pdf_text\n",
    "\n",
    "\n",
    "def find_text_with_regexes(regex_dict, text):\n",
    "    # Create an empty dictionary to hold the results\n",
    "    results = {}\n",
    "    my_text = '\\t'.join(text)\n",
    "    print(my_text)\n",
    "    # Loop through each label and regex in the dictionary\n",
    "    for label, regex in regex_dict.items():\n",
    "        # Use regex to search for the pattern in the text\n",
    "        # print(f\"Searching for label '{label}' with regex '{regex}'\")\n",
    "        match = re.search(regex, my_text)\n",
    "\n",
    "        # If a match is found, add it to the results dictionary with the current label as the key\n",
    "        if match:\n",
    "            # print(f\"Found match '{match.group(2)}'\")\n",
    "            results[label] = match.group(2).strip()\n",
    "        # else:\n",
    "        #     print(f\"{label} not found\")\n",
    "\n",
    "    # Return the results dictionary\n",
    "    return results\n",
    "\n",
    "\n",
    "regex_dict = {\"book_title\":\"(?i)(book\\s+title|title(?<!Subtitle))(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"subtitle\":\"(?i)(book\\ssub\\s?title|sub\\s?title)(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"series\":\"(?i)(Series)(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"author\":\"(Name:\\s?Role:\\s?Email:|Author\\/Eds|Author(?:s?)(?:\\(s\\))?|contact\\sauthor(?:s?)(?:\\(s\\))|(?<!Anthem\\s)Editor(?:s)?(?:\\(s\\))?|Contributor(?:s?)(?:\\(s\\)))(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"author_mailid\":\"(?i)((?:Email(?::|,|-|–)?(?:\\t).+?\\t?(?:Editor|Author)\\t?)|\\(email|Author Contact.+?)[:]?\\s?([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\",\n",
    "\"isbn\":\"(?i)(ISBN)\\s?(?<!E-ISBN)(?:[:,-–])?(.+?)(?<=\\t)\",\n",
    "\"hardback_isbn\":\"(?i)(ISBN\\s\\(hardback\\)|ISBN\\s\\(hard|cloth\\)|hardback)\\s?(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"paperback_isbn\":\"(?i)(ISBN\\s\\(paperback\\)|ISBN\\s\\(paper\\)|paperback|NIPPOD)\\s?(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"epub_isbn\":\"(?i)(ePub\\sebook|ePub|ISBN\\s\\(elec\\.\\)):\\s?(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"epub2_isbn\":\"(?i)(XML)\\s?(?::|,|-|–)\\s?(.+?(?=\\t))\",\n",
    "\"ebook_master\":\"(?i)(ePDF|Web\\sPDF|(?<!ePub\\s)ebook|(?<!print-ready\\stext\\s)PDF|E-ISBN)\\s?(?::|,|-|–)\\s?(.+?(?=\\t))\",\n",
    "\"isbn_opt\":\"(?i)(\\s)?(.+?(?=\\t))\\((ISBN(?:\\s?))\\)\",\n",
    "\"hardback_isbn_opt\":\"(?i)(\\s)?(.+?(?=\\t))\\((Hardback)\\)\",\n",
    "\"paperback_isbn_opt\":\"(?i)(\\s)?(.+?(?=\\t))\\((paperback)\\)\",\n",
    "\"epub_isbn_opt\":\"(?i)(\\s)?(.+?(?=\\t))\\((ePub)\\)\",\n",
    "\"ebook_master_opt\":\"(?i)(\\s)?(.+?(?=\\t))\\((Web\\sPDF)\\)\",\n",
    "\"po_number\":\"(?i)(PO\\sNumber|MUP\\skeycode|Purchase\\sOrder(?:\\sNumber)?|P\\.O\\.\\s\\#)\\s?(?::|,|-|–)?\\s?([\\w\\d]+?(?=\\t)(he\\/him)?)\",\n",
    "\"page_count\":\"(?i)(Estimated\\sprint\\sextent|Estimated\\spage\\sextent|Est\\.\\sfinal\\spp\\scount|Extent)\\s?(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"project_startDate\":\"(?i)(Handover\\sdate|start\\sdate|date(?<!Requested\\sfinals\\sdate)(?!\\sdue|s\\sare styled|\\sreferences used in|\\sreferences).\\))\\s?(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"project_endDate\":\"(?i)(Target\\ssend-to-print\\sdate\\sFinal\\sdue|Requested\\sfinals\\sdate|Target\\ssend-to-print\\sdate|Schedule:\\sReturn\\smanuscript\\sby|Final\\sPDF\\s\\+\\sPOD\\sfiles|Printer\\sDate|Final\\sPDF\\sdue|End date)\\s?(?::|,|-|–)?\\s?(?:late\\s|spring|Publication date)?(.+?(?=\\t))\",\n",
    "\"production_editor\":\"(Bloomsbury\\sProduction\\scontact|Anthem\\sEditor|Production\\sEditor|^From|Bloomsbury Production Editor|PE)\\s?(?::|,|-|–)(.+?(?=\\t))(,\\s?Rowman\\s\\&\\sLittlefield)?\",\n",
    "\"project_manager\":\"(?i)(To)\\s?(?:[:,-–])?(?:[:,-–]?\\s+)?(.+?(?=\\t))(,\\s?Deanta Global)\",\n",
    "\"text_design\":\"(?i)(Text\\sdesign\\stemplate|Text\\sdesign|^template|Design\\stemplate|Layout\\sstyle.+?X|Design)\\s?(?::|,|-|–)?\\s?(.+?(?=\\t))\",\n",
    "\"trim_size\":\"(?i)(TPS|Page\\strim|Format|Trim\\ssize|Trim)\\s?(?:[:,-–]?)(?:\\s{1,})?(\\d{1,3}\\s?(?:\\.?\\d{1,3}?)?\\s?x\\s?\\d{1,3}\\.?(?:\\.?\\d{1,3}?)?\\s?)\\s?(?:mm|in)?(?:.+?)(?=\\t)\",\n",
    "\"Indexer\":\"(?i)(Index[:]\\sYes\\s☒\\sNo\\s☐\\s+Compiled\\sby[:]\\s+)(.+?(?=\\t))\",\n",
    "\"productioneditor_mailid\":\"(?i)(, Email(?::|,|-|–)?)(.+?(?=\\t))\",\n",
    "\"binding\":\"(?i)(Price\\s+\\d+\\s[-]\\s)(.+?(?=\\t))\",\n",
    "\"published_date\":\"(?i)(Target\\spublication\\sdate)\\s?(?::|,|-|–)?\\s??(.+?(?=\\t))\"}\n",
    "\n",
    "\n",
    "results = find_text_with_regexes(regex_dict, text)\n",
    "\n",
    "# cleanup the extracted texts\n",
    "\n",
    "def metadata_cleanup(results):    \n",
    "\n",
    "    # removing \":\" from isbn\n",
    "    pattern = r\"\\d+\\.\\d+ - First Printing|£\\d+\\.\\d+ GBP|[-\\s:]|\\(cloth\\)|\\(paper\\)|\\(elec\\.\\)|\\bN(?:\\/)?A\\b|\\(delayed\\)|\\(NIPPOD\\)|ISBN|Paperback|£\\d+\\.\\d+GBP\"\n",
    "\n",
    "    # List of keys for which the ISBN values need to be cleaned\n",
    "    isbn_keys_to_clean = [\"isbn\", \"hardback_isbn\", \"paperback_isbn\", \"epub_isbn\", \"epub2_isbn\", \"ebook_master\", \"isbn_opt\", \"hardback_isbn_opt\", \"paperback_isbn_opt\", \"epub_isbn_opt\", \"ebook_master_opt\"]\n",
    "    # Clean ISBN values for each key in 'keys_to_clean'\n",
    "    for key in isbn_keys_to_clean:\n",
    "        if key in results and results[key] is not None:\n",
    "            print(f\"harback ISBN: {results[key]}\")\n",
    "            results[key] = re.sub(pattern, \"\", results[key])\n",
    "\n",
    "\n",
    "    # replacing the \"isbn\" with \"hardback_isbn\"\n",
    "    try:\n",
    "        if not results[\"isbn\"]:\n",
    "            _default_hardback_isbn = results.get(\"hardback_isbn\", \"\")\n",
    "            results[\"isbn\"]= _default_hardback_isbn\n",
    "            if not results[\"isbn\"]:\n",
    "                _default_paperback_isbn = results.get(\"paperback_isbn\", \"\")\n",
    "                results[\"isbn\"]= _default_paperback_isbn\n",
    "    except:\n",
    "        print(\"ISBN is not found\")\n",
    "\n",
    "\n",
    "    # removing prefix from \"author\"\n",
    "    authors_pattern = \"(?i)Dr(\\.)?|\\bEdited by\\b|,\\sRowman\\s\\&\\sLittlefield\\s(?:Publishing\\sGroup)?|\\(R\\&L template\\)|\\bN(?:\\/)?A\\b|\\bProfessor\\b|Handbook format|\\[(?:under|over)\\s\\d+K\\]|\\(she\\/her\\)\"\n",
    "\n",
    "    person_keys_to_clean = [\"author\", \"production_editor\", \"project_manager\",\"text_design\",\"subtitle\",\"series\"]\n",
    "    for key in person_keys_to_clean:\n",
    "        if key in results and results[key] is not None:\n",
    "            results[key] = re.sub(authors_pattern, \"\", results[key])\n",
    "\n",
    "    return results\n",
    "\n",
    "updated_metadata = metadata_cleanup(results)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the dictionary to a list of lists (rows) for tabulate\n",
    "data = [[key, value] for key, value in updated_metadata.items()]\n",
    "\n",
    "# Print the dictionary as a table\n",
    "print(tab.tabulate(data, headers=[\"Key\", \"Metadata\"], tablefmt=\"fancy_grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
